{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWmc_s2ezvU0"
      },
      "source": [
        "# Run streamlit app from a Google Colab Notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvlYkCQ9vFiy"
      },
      "source": [
        "#!pip install -q streamlit\n",
        "#!pip install miceforest\n",
        "#!pip install bayesian-optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('model.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "model.feature_names.sort_values()"
      ],
      "metadata": {
        "id": "an2C3c_0Ooi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b10b7f9-49cc-496b-fb13-1367b2402cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'age_group_under 30', 'avg_balance',\n",
              "       'avg_days_since_credit_update', 'avg_receivable_sum',\n",
              "       'avg_weighted_receivable_sum', 'code_gender_F', 'code_gender_M',\n",
              "       'count_active_credits', 'days_id_publish', 'days_last_phone_change',\n",
              "       'debt_to_credit_ratio', 'flag_emp_phone', 'max_balance',\n",
              "       'max_weighted_balance', 'mean_weighted_balanced',\n",
              "       'name_education_type_Higher education',\n",
              "       'name_education_type_Secondary / secondary special',\n",
              "       'name_income_type_Pensioner', 'name_income_type_Working',\n",
              "       'occupation_type_Laborers', 'organization_type_other',\n",
              "       'perc_adult_life_employed', 'perc_late', 'perc_underpaid',\n",
              "       'prop_credit_rejected', 'reg_city_not_work_city', 'total_rejected_apps',\n",
              "       'total_weighted_receivable_sum', 'yrs_employed'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waCfwniZOow8"
      },
      "source": [
        "## Create a streamlit app example\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "\n",
        "from classes import *\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define function to calculate age group\n",
        "def calculate_age_group(age):\n",
        "    if age < 30:\n",
        "        return 'under 30'\n",
        "    elif age < 45:\n",
        "        return '30-45'\n",
        "    elif age < 60:\n",
        "        return '45-60'\n",
        "    else:\n",
        "        return '60+'\n",
        "\n",
        "# Define function to calculate debt-to-credit ratio\n",
        "def calculate_debt_to_credit_ratio(credit_debt, credit_sum):\n",
        "    if credit_sum == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return credit_debt / credit_sum\n",
        "\n",
        "# Function to calculate statistics on balance and weighted balance\n",
        "def calculate_balance_statistics(balance_string):\n",
        "    # Remove whitespace and split the balance string\n",
        "    balance_values = [value.strip() for value in balance_string.split(',')]\n",
        "\n",
        "    # Convert balance values to floats and replace non-numeric inputs with NAs\n",
        "    balances = []\n",
        "    for value in balance_values:\n",
        "        try:\n",
        "            balance = float(value)\n",
        "            balances.append(balance)\n",
        "        except ValueError:\n",
        "            balances.append(np.nan)\n",
        "\n",
        "    # Calculate statistics\n",
        "    average_balance = np.nanmean(balances)\n",
        "    maximum_balance = np.nanmax(balances)\n",
        "    minimum_balance = np.nanmin(balances)\n",
        "\n",
        "    # Calculate weighted statistics\n",
        "    weighted_balances = [balance / (i + 1) for i, balance in enumerate(balances) if not np.isnan(balance)]\n",
        "    average_weighted_balance = np.nanmean(weighted_balances)\n",
        "    maximum_weighted_balance = np.nanmax(weighted_balances)\n",
        "    minimum_weighted_balance = np.nanmin(weighted_balances)\n",
        "\n",
        "    return {\n",
        "        'average_balance': average_balance,\n",
        "        'maximum_balance': maximum_balance,\n",
        "        'minimum_balance': minimum_balance,\n",
        "        'average_weighted_balance': average_weighted_balance,\n",
        "        'maximum_weighted_balance': maximum_weighted_balance,\n",
        "        'minimum_weighted_balance': minimum_weighted_balance\n",
        "    }\n",
        "\n",
        "# Define function to perform prediction\n",
        "def predict_default(features):\n",
        "  ##change days id publish from yrs to days and make negative\n",
        "\n",
        "  # Here put the logic for your prediction\n",
        "  prediction = 0\n",
        "  return prediction\n",
        "\n",
        "# Define your Streamlit app\n",
        "def run():\n",
        "\n",
        "    # Using markdown for formatted text\n",
        "    st.markdown(\"# Loan Default Prediction App\")\n",
        "    \n",
        "    # Create a sidebar for user input\n",
        "    st.sidebar.markdown('## User Input')\n",
        "    \n",
        "    # Get user inputs from Streamlit sliders and dropdowns\n",
        "    feature_dict = {}\n",
        "    feature_dict['age'] = st.sidebar.slider('Age', min_value=18, max_value=100, value=30)\n",
        "    feature_dict['balance_string'] = st.sidebar.text_input('Input monthly credit card balances separated by a comma, starting with the most recent',\n",
        "                                              value='5000, 6000, 5500, 7000, 8000')\n",
        "    feature_dict['average_days_since_credit_update'] = st.sidebar.slider('Average Days Since Credit Update (across all forms of credit)', \n",
        "                                                            min_value=0.0, max_value=365.0, value=30.0)\n",
        "    feature_dict['average_receivable_sum'] = st.sidebar.slider('Average Receivable Sum', min_value=0.0, max_value=100000.0, value=10000.0)\n",
        "    feature_dict['average_weighted_receivable_sum'] = st.sidebar.slider('Average Weighted Receivable Sum', min_value=0.0, max_value=100000.0, value=8000.0)\n",
        "    feature_dict['gender'] = st.sidebar.selectbox('Gender', ['Male', 'Female','other'])\n",
        "    feature_dict['count_active_credits'] = st.sidebar.slider('Number of total active credits (including credit cards, car loans, mortgages)', \n",
        "                                                min_value=0, max_value=50, value=4)\n",
        "    feature_dict['days_id_publish'] = st.sidebar.slider('How many years you changed the ID document that you will use for the loan application?',\n",
        "                                            min_value=0, max_value=100, value=5)\n",
        "    feature_dict['days_last_phone_change'] = st.sidebar.slider('How many years since your phone number was last changed?', \n",
        "                                                    min_value=0, max_value=10, value=2)\n",
        "    feature_dict['credit_debt'] = st.sidebar.slider('Credit Debt', min_value=0.0, max_value=100000.0, value=5000.0)\n",
        "    feature_dict['credit_sum'] = st.sidebar.slider('Total open credit (US$) ', min_value=0.0, max_value=100000.0, value=20000.0)\n",
        "    feature_dict['flag_emp_phone'] = st.sidebar.selectbox('Do you have a work phone?', ['Yes', 'No'])\n",
        "    feature_dict['education_type'] = st.sidebar.selectbox('Education Type', ['Higher education', 'Secondary / secondary special'])\n",
        "    feature_dict['income_type'] = st.sidebar.selectbox('Income Type', ['Pensioner', 'Working'])\n",
        "    feature_dict['occupation_type'] = st.sidebar.selectbox('Occupation Type', ['Laborers'])\n",
        "    feature_dict['organization_type'] = st.sidebar.selectbox('Organization Type', ['Other'])\n",
        "    feature_dict['total_rejected_credit'] = st.sidebar.slider('Total Rejected Credit', min_value=0.0, max_value=100000.0, value=8000.0)\n",
        "    feature_dict['total_accepted_credit'] = st.sidebar.slider('Total Accepted Credit', min_value=0.0, max_value=100000.0, value=15000.0)\n",
        "\n",
        "\n",
        "    # Calculate age group\n",
        "    age_group = calculate_age_group(feature_dict['age'])\n",
        "    \n",
        "    # Calculate debt-to-credit ratio\n",
        "    debt_to_credit_ratio = calculate_debt_to_credit_ratio(feature_dict['credit_debt'],\n",
        "                                                          feature_dict['credit_sum'])\n",
        "    \n",
        "    # Calculate balance statistics\n",
        "    balance_statistics = calculate_balance_statistics(feature_dict['balance_string'])\n",
        "    \n",
        "    \n",
        "    result = \"\"\n",
        "    \n",
        "    # When 'Predict' is clicked, make the prediction and store it\n",
        "    if st.sidebar.button(\"Predict\"):\n",
        "        #result = predict_default(features)\n",
        "\n",
        "        # Display the prediction result\n",
        "        #st.success(f'The probability of default is: {result}')\n",
        "        st.markdown(\"## Here are your selected features:\")\n",
        "        st.markdown(feature_dict)\n",
        "    else:\n",
        "        st.markdown('## Enter the feature values and click \"Predict\" to get the default probability.')\n",
        "    \n",
        "if __name__=='__main__':\n",
        "    run()\n"
      ],
      "metadata": {
        "id": "meJ36PefNftd",
        "outputId": "b0bbb8d3-8602-41c9-9a45-896636018ff5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZlEJkmSOoxC"
      },
      "source": [
        "## Install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7094361-2ced-4079-9e49-e93e7b00ea90",
        "id": "ZAyqQCQVOoxC"
      },
      "source": [
        "!npm install localtunnel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.261s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run streamlit in background"
      ],
      "metadata": {
        "id": "kccYE2lkN20y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "Zv912rRAN0fs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04db646f-22d4-4d3e-ea4a-2976ca677853"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.125.152.233\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.59s\n",
            "your url is: https://olive-parrots-smash.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile classes.py\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import gc #free up memory\n",
        "import matplotlib.pyplot as plt\n",
        "import miceforest as mf ##forest based imputation\n",
        "\n",
        "\n",
        "import xgboost as xgb\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score, balanced_accuracy_score,precision_recall_curve,roc_curve, auc\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(self, df, target,is_test=False, scaler=None, trained_cols = None):\n",
        "      input_df = df.copy()\n",
        "      self.is_test = is_test\n",
        "      self.target = target\n",
        "      self.X_train = None\n",
        "      self.X_test = None\n",
        "      self.X_eval = None\n",
        "      self.y_eval = None\n",
        "      self.y_train = None\n",
        "      self.y_test = None\n",
        "      self.preprocessed = False\n",
        "      self.scaler = scaler\n",
        "      self.label_encoders = None\n",
        "      if self.is_test:\n",
        "        self.y = None\n",
        "        self.X = input_df\n",
        "      else:\n",
        "        self.y = input_df[self.target]\n",
        "        self.X = input_df.drop(columns=[self.target])\n",
        "\n",
        "    ##method to pre-process the df\n",
        "    def preprocess(self, impute_dict=None, final_X_cols= None,\n",
        "                   imputation_kernel_iterations = 4, imputation_kernel_ntrees = 50):\n",
        "       # Basic imputations\n",
        "      if impute_dict is not None:\n",
        "        print(f'Performing basic imputations based on {len(impute_dict)} features supplied impute_dict')\n",
        "        for col, strategy in impute_dict.items():\n",
        "          if col not in self.X.columns:\n",
        "            print(f\"Skipping imputation for column '{col}' as it does not exist in the dataset.\")\n",
        "            continue\n",
        "          if strategy == 'mean':\n",
        "              self.X[col].fillna(self.X[col].mean(), inplace=True)\n",
        "          elif strategy == 'median':\n",
        "              self.X[col].fillna(self.X[col].median(), inplace=True)\n",
        "          elif isinstance(strategy, str) and strategy.startswith('percentile_'):\n",
        "              percentile = float(strategy.split('_')[1])\n",
        "              self.X[col].fillna(self.X[col].quantile(percentile / 100), inplace=True)\n",
        "          elif isinstance(strategy, (int, float)):\n",
        "              self.X[col].fillna(strategy, inplace=True)\n",
        "          else:\n",
        "              raise ValueError(f\"Invalid imputation strategy for column '{col}'.\")\n",
        "    #find numeric columns\n",
        "      numeric_cols = self.X.select_dtypes(include=['int64', 'float64']).columns\n",
        "      #print(numeric_cols)\n",
        "\n",
        "      ## smart impuations - decision tree based method\n",
        "      count_NA = self.X.isna().sum()\n",
        "      remaining_NA_cols = count_NA[count_NA>0].shape[0]\n",
        "      #print(remaining_NA_cols)\n",
        "      if  remaining_NA_cols> 0:\n",
        "        print(f'Performing decision-tree based imputations of {remaining_NA_cols} remaining features with missing data')\n",
        "        kernal = mf.ImputationKernel(\n",
        "            self.X[numeric_cols],\n",
        "            random_state=42\n",
        "            )\n",
        "        # Run the MICE algorithm for 2 iterations\n",
        "        kernal.mice(iterations=imputation_kernel_iterations,\n",
        "                    n_estimators=imputation_kernel_ntrees)\n",
        "        X_numeric_imputed = kernal.complete_data()\n",
        "        self.X[numeric_cols] = X_numeric_imputed\n",
        "      \n",
        "      ##scale numeric cols\n",
        "      print('Scaling numeric data')\n",
        "      if self.scaler is None:\n",
        "        self.scaler = StandardScaler()\n",
        "        self.X[numeric_cols] = self.scaler.fit_transform(self.X[numeric_cols])\n",
        "      else:\n",
        "        scaler_trained_features = self.scaler.feature_names_in_\n",
        "        missing_cols = set(scaler_trained_features) - set(self.X.columns)\n",
        "        if len(missing_cols) >0:\n",
        "          print(f\"{len(missing_cols)} variables that scaler was originally trained on are missing - will replace with zeros\")\n",
        "          for col in missing_cols:\n",
        "            self.X[col] = 0\n",
        "        self.X[scaler_trained_features] = self.scaler.transform(self.X[scaler_trained_features])\n",
        "  \n",
        "      \n",
        "      # Perform label encoding for binary columns\n",
        "      print('One-hot-encoding categorical vars')\n",
        "      binary_cols = [col for col in self.X.columns if self.X[col].nunique() == 2]\n",
        "      self.label_encoders = {}\n",
        "\n",
        "      # Label encode binary columns\n",
        "      for col in binary_cols:\n",
        "          label_encoder = LabelEncoder()\n",
        "          self.X[col] = label_encoder.fit_transform(self.X[col])\n",
        "          # Store the label encoder for later use\n",
        "          self.label_encoders[col] = label_encoder\n",
        "      \n",
        "      # Perform one-hot encoding for categorical columns\n",
        "      categorical_cols = [col for col in self.X.columns if self.X[col].dtype == 'object' and col not in binary_cols]\n",
        "      self.X = pd.get_dummies(self.X, columns=categorical_cols)\n",
        "\n",
        "      if final_X_cols is not None:\n",
        "        print('splicing dataset to include only final_X_cols columns')\n",
        "        missing_cols = set(final_X_cols) - set(self.X.columns)\n",
        "        for col in missing_cols:\n",
        "          self.X[col] = 0\n",
        "        self.X = self.X[final_X_cols]\n",
        "\n",
        "      self.preprocessed = True\n",
        "    \n",
        "\n",
        "    def split_data(self, test_size=0.15,eval_size = 0.15, random_state=42):\n",
        "        if not self.preprocessed:\n",
        "          raise RuntimeError(\"Data has not been preprocessed. Please run the preprocess method.\")\n",
        "        \n",
        "        if self.is_test:\n",
        "          raise RuntimeError(\"Cannot run split_data() method on a test set\")\n",
        "\n",
        "        \n",
        "        X_train_eval, self.X_test, y_train_eval, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=test_size, random_state=random_state\n",
        "        )\n",
        "\n",
        "        eval_split_size = eval_size/(1-test_size)\n",
        "        self.X_train, self.X_eval, self.y_train, self.y_eval = train_test_split(\n",
        "            X_train_eval, y_train_eval, test_size=eval_split_size, random_state=random_state)\n",
        "\n",
        "        print(f\"{self.X_train.shape[0]} training samples, {self.X_eval.shape[0]} evaluation samples and {self.X_test.shape[0]} testing samples\")\n",
        "        print(f\"{self.y_train.sum()} ({self.y_train.mean()*100:.3f}%) positives in training set\")\n",
        "        print(f\"{self.y_eval.sum()} ({self.y_eval.mean()*100:.3f}%) positives in evaluation set\")\n",
        "        print(f\"{self.y_test.sum()} ({self.y_test.mean()*100:.3f})% positives in testing set\")\n",
        "\n",
        "\n",
        "\n",
        "class Model:\n",
        "    ##initialise the model class - taking instance of `Dataset` class an input\n",
        "    def __init__(self, dataset_class):\n",
        "\n",
        "      \"\"\"\n",
        "      Initialize the model.\n",
        "      Args:\n",
        "        dataset_class (Dataset): An instance of the Dataset class. It should have attributes X_train, X_test, y_train, y_test.\n",
        "      \"\"\"\n",
        "      if not isinstance(dataset_class.X_train, pd.DataFrame) or not isinstance(dataset_class.X_test, pd.DataFrame):\n",
        "          raise TypeError(\"X_train and X_test must be pandas DataFrames\")\n",
        "      if not isinstance(dataset_class.y_train, pd.Series) or not isinstance(dataset_class.y_test, pd.Series):\n",
        "          raise TypeError(\"y_train and y_test must be pandas Series\")\n",
        "      ## take train-eval-test split datasets from input\n",
        "      self.X_train = dataset_class.X_train\n",
        "      self.X_eval = dataset_class.X_eval\n",
        "      self.X_test = dataset_class.X_test\n",
        "      self.y_train = dataset_class.y_train\n",
        "      self.y_eval = dataset_class.y_eval\n",
        "      self.y_test = dataset_class.y_test\n",
        "      ##set up attributes to be used later\n",
        "      self.y_pred = None\n",
        "      self.xgboost_params = None\n",
        "      self.feature_names = dataset_class.X_train.columns\n",
        "      self.model = None\n",
        "      self.is_model_trained = False\n",
        "\n",
        "\n",
        "    ##define method to select best n features in dataset\n",
        "    ##basically a wrapper for SelectKBest from sklearn\n",
        "\n",
        "    def select_features(self, num_features):\n",
        "\n",
        "      \"\"\"\n",
        "      Select the best features from the dataset using SelectKBest from sklearn.\n",
        "      Args:\n",
        "        num_features (int or float): The number of features to select.\n",
        "      \"\"\"\n",
        "      if not isinstance(num_features, (int, float)):\n",
        "          raise TypeError(\"num_features must be an int or float\")\n",
        "\n",
        "      # Create VarianceThreshold object\n",
        "      constant_filter = VarianceThreshold(threshold=0)\n",
        "\n",
        "      # Fit VarianceThreshold object to data, then get the support mask\n",
        "      constant_filter.fit(self.X_train)\n",
        "      constant_support = constant_filter.get_support()\n",
        "\n",
        "      # Get the columns with >0 variance\n",
        "      non_constant_columns = self.X_train.columns[constant_support]\n",
        "\n",
        "      # Round num_features to the nearest integer\n",
        "      num_features = max(min(round(num_features), len(non_constant_columns)), 2)\n",
        "\n",
        "      # Perform feature selection on columns with >0 variance\n",
        "      selector = SelectKBest(score_func=f_classif, k=num_features)\n",
        "      selector.fit(self.X_train[non_constant_columns], self.y_train)\n",
        "      mask = selector.get_support()\n",
        "\n",
        "      # Get selected feature indices\n",
        "      selected_indices = np.where(mask)[0]\n",
        "\n",
        "      # Map indices back to column names\n",
        "      self.feature_names = non_constant_columns[selected_indices]\n",
        "\n",
        "      #print(f'Completed feature selection for best {num_features}')\n",
        "\n",
        "\n",
        "    ## defne method to train the classifier\n",
        "    def train_model(self, \n",
        "                    xgboost_params,\n",
        "                    print_training_evaluation=False ,\n",
        "                    num_boost_round=700,\n",
        "                    early_stopping_rounds=20):\n",
        "      \"\"\"Train the XGBoost classifier model.\n",
        "      Args:\n",
        "          xgboost_params (dict): A dictionary of XGBoost parameters.\n",
        "          print_training_evaluation (bool, optional): Whether to print the training evaluation. Defaults to False.\n",
        "          num_boost_round (int, optional): The number of boosting rounds or trees to build. Defaults to 700.\n",
        "          early_stopping_rounds (int, optional): Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. Defaults to 20.\n",
        "      \"\"\"\n",
        "      self.xgboost_params = xgboost_params\n",
        "\n",
        "      ##create xgb Matrix objects for datasets\n",
        "      #print('subsetting datasets of selected')\n",
        "      x_train = self.X_train.loc[:, self.feature_names]\n",
        "      x_eval = self.X_eval.loc[:, self.feature_names]\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      #print('making xgb.DMatrix objects from datasets')\n",
        "      dtrain = xgb.DMatrix(x_train, label=self.y_train)\n",
        "      ##\n",
        "      deval = xgb.DMatrix(x_eval, label = self.y_eval)\n",
        "      self.model = xgb.train(self.xgboost_params,\n",
        "                             dtrain = dtrain, \n",
        "                             evals=[(deval, 'eval')],\n",
        "                             verbose_eval=print_training_evaluation,\n",
        "                             num_boost_round=num_boost_round,\n",
        "                             early_stopping_rounds=early_stopping_rounds )\n",
        "      print(f'Model training completed on {self.X_train.loc[:, self.feature_names].shape[1]} features. Best evaluation score (ROC-AUC:{self.model.best_score:.3f}) was obtained at iteration {self.model.best_iteration}')\n",
        "      self.is_model_trained = True\n",
        "\n",
        "    def _objective_function(self,\n",
        "                            learning_rate,\n",
        "                            max_depth, \n",
        "                            scale_pos_weight,\n",
        "                            subsample,\n",
        "                            colsample_bytree,\n",
        "                            colsample_bynode,\n",
        "                            min_child_weight, \n",
        "                            num_features):\n",
        "      \"\"\"\n",
        "      Defines the objective function for the XGBoost model.\n",
        "      This will be utilised in a bayesian search for optimal hyperparams\n",
        "      See `bayesian_hyperparam_optimisation()` method.\n",
        "\n",
        "      Args:\n",
        "          learning_rate (float): Learning rate for the XGBoost model.\n",
        "          max_depth (int): Maximum depth of a tree for the XGBoost model.\n",
        "          scale_pos_weight (float): Controls the balance of positive and negative weights.\n",
        "          subsample (float): Subsample ratio of the training instances.\n",
        "          colsample_bytree (float): Subsample ratio of columns when constructing each tree.\n",
        "          colsample_bynode (float): Subsample ratio of columns for each node.\n",
        "          min_child_weight (int): Minimum sum of instance weight (hessian) needed in a child.\n",
        "          num_features (int): The number of features in the dataset.\n",
        "      \"\"\"\n",
        "      \n",
        "      if self.is_model_trained:\n",
        "          raise RuntimeError(\"Model cannot be trained before hyperparam_tuned\")\n",
        "      xgb_params = {\n",
        "          'max_depth': int(max_depth),\n",
        "          'learning_rate': learning_rate,\n",
        "          'objective': 'binary:logistic',\n",
        "          'eval_metric': 'auc',\n",
        "          'scale_pos_weight': scale_pos_weight,\n",
        "          'subsample':subsample,\n",
        "          'colsample_bynode':colsample_bynode,\n",
        "          'min_child_weight': min_child_weight,\n",
        "          'colsample_bytree':colsample_bytree,\n",
        "          'eval_metric':'auc'\n",
        "          }\n",
        "      #select features if number inputted lower than number of features\n",
        "      if num_features < self.X_train.shape[1]:\n",
        "        self.select_features(num_features)\n",
        "      self.train_model(xgb_params,\n",
        "                       num_boost_round=250,\n",
        "                       early_stopping_rounds=5,\n",
        "                       print_training_evaluation=False)\n",
        "      \n",
        "      self.is_model_trained = False\n",
        "          \n",
        "          # Return ROC-AUC value for BayesianOptimization to maximise\n",
        "      return self.model.best_score\n",
        "\n",
        "    def bayesian_hyperparam_optimisation(self, pbounds, \n",
        "                                         start_hyperparam = None,\n",
        "                                         initial_random_search_iterations = 8,\n",
        "                                         bayesian_search_iterations = 20,\n",
        "                                         retrain_with_best_params=True):\n",
        "      \"\"\"Perform Bayesian hyperparameter optimization.\n",
        "\n",
        "      Args:\n",
        "          pbounds (dict): Dictionary containing hyperparameter bounds for the optimization.\n",
        "          start_hyperparam (dict, optional): Dictionary containing initial hyperparameters. Defaults to None.\n",
        "          initial_random_search_iterations (int, optional): Number of initial iterations to perform random search. Defaults to 8.\n",
        "          bayesian_search_iterations (int, optional): Number of iterations to perform Bayesian optimization. Defaults to 20.\n",
        "          retrain_with_best_params (bool, optional): Whether to retrain the model with the best parameters found. Defaults to True.\n",
        "      \"\"\"\n",
        "\n",
        "      print('Validating hyperparamater inputs')\n",
        "      hyper_params = ['max_depth','learning_rate','scale_pos_weight',\n",
        "                      'subsample','colsample_bynode','min_child_weight',\n",
        "                      'colsample_bytree','num_features']\n",
        "      required_params = hyper_params[0:7]\n",
        "      all_present = all(item in pbounds for item in required_params)\n",
        "\n",
        "      if not all_present:\n",
        "        raise RuntimeError(\"Some required hyperparams for Bayesian Optimisation are missing\")\n",
        "      if 'num_features' not in pbounds:\n",
        "        ##if num features not set, set it a high number.\n",
        "        pbounds['num_features'] = (1e6, 1e6+1)\n",
        "\n",
        "      \n",
        "      \n",
        "      optimizer = BayesianOptimization(\n",
        "          f=self._objective_function,\n",
        "          ##subset input for allowed hyperparams\n",
        "          pbounds={param: pbounds[param] for param in hyper_params},\n",
        "          random_state=42,\n",
        "          verbose=2\n",
        "          )\n",
        "      \n",
        "      if start_hyperparam is not None:\n",
        "        all_present_start_point = all(item in start_hyperparam for item in required_params)\n",
        "        if all_present_start_point:\n",
        "          print(f'Starting optimisation at specified paramaters: {start_hyperparam}')\n",
        "          optimizer.probe({param: start_hyperparam[param] for param in pbounds.keys()})\n",
        "      \n",
        "      print('Performing hyperparamater optimisation')\n",
        "      optimizer.maximize(init_points=initial_random_search_iterations, \n",
        "                         n_iter=bayesian_search_iterations)\n",
        "      \n",
        "      best_params = optimizer.max['params']\n",
        "      best_params['max_depth'] = int(best_params['max_depth'])\n",
        "      best_params['objective'] ='binary:logistic'\n",
        "      best_params['eval_metric']='auc'\n",
        "\n",
        "      del best_params[\"num_features\"]\n",
        "\n",
        "      if retrain_with_best_params:\n",
        "        self.train_model(best_params,\n",
        "                          num_boost_round=1250,\n",
        "                          early_stopping_rounds=50,\n",
        "                          print_training_evaluation=False)\n",
        "      \n",
        "      return optimizer\n",
        "\n",
        "    def roc_auc(self):\n",
        "      \"\"\"Calculate ROC AUC score for the model.\n",
        "      \"\"\"\n",
        "\n",
        "      if not self.is_model_trained:\n",
        "          raise RuntimeError(\"Model must be trained before it can be evaluated\")\n",
        "      if self.y_pred is None:\n",
        "        dtest = xgb.DMatrix(self.X_test.loc[:, self.feature_names])\n",
        "        self.y_pred = self.model.predict(dtest)\n",
        "\n",
        "      roc_auc = roc_auc_score(self.y_test, self.y_pred)\n",
        "\n",
        "      return roc_auc\n",
        "    \n",
        "    def evaluate_model(self, opt_thresh_search_precision=0.01):\n",
        "      \"\"\"\n",
        "      Evaluate the model performance using various metrics.\n",
        "      This method also searches for the optimal threshold for binary target classification where F1 score is highest\n",
        "      Args:\n",
        "          opt_thresh_search_precision (float, optional): Precision for threshold search. Defaults to 0.01.\n",
        "      \"\"\"\n",
        "\n",
        "      if not self.is_model_trained:\n",
        "          raise RuntimeError(\"Model must be trained before it can be evaluated\")\n",
        "      if self.y_pred is None:\n",
        "        dtest = xgb.DMatrix(self.X_test.loc[:, self.feature_names])\n",
        "        self.y_pred = self.model.predict(dtest)\n",
        "      ##select best threshold for determining \n",
        "      best_threshold = 0\n",
        "      best_f1 = 0\n",
        "      \n",
        "      # Iterate over different threshold values\n",
        "      for threshold in np.arange(0.01, 1.0, opt_thresh_search_precision):\n",
        "          y_pred_binary = (self.y_pred >= threshold).astype(int)\n",
        "          f1 = f1_score(self.y_test, y_pred_binary)\n",
        "\n",
        "          if f1 > best_f1:\n",
        "              best_f1 = f1\n",
        "              best_threshold = threshold\n",
        "      \n",
        "      # Convert predicted probabilities to binary predictions based on the best threshold\n",
        "      y_pred_binary = (self.y_pred >= best_threshold).astype(int)\n",
        "      \n",
        "      # Calculate accuracy metrics\n",
        "      accuracy = accuracy_score(self.y_test, y_pred_binary)\n",
        "      precision = precision_score(self.y_test, y_pred_binary)\n",
        "      recall = recall_score(self.y_test, y_pred_binary)\n",
        "      specificity = recall_score(self.y_test, y_pred_binary, pos_label=0)\n",
        "      roc_auc = self.roc_auc()\n",
        "      balanced_accuracy = balanced_accuracy_score(self.y_test, y_pred_binary)\n",
        "      \n",
        "      # Print the metrics\n",
        "      print(f\"Optimal Threshold: {best_threshold:.3f}\")\n",
        "      print(f\"F1 Score: {best_f1:.3f}\")\n",
        "      print(f\"Accuracy: {accuracy:.3f}\")\n",
        "      print(f\"Precision: {precision:.3f}\")\n",
        "      print(f\"Recall (Sensitivity): {recall:.3f}\")\n",
        "      print(f\"Specificity (True Negative Rate): {specificity:.3f}\")\n",
        "      print(f\"ROC AUC Score: {roc_auc:.3f}\")\n",
        "      print(f\"Balanced Accuracy: {balanced_accuracy:.3f}\")\n",
        "\n",
        "    \n",
        "    def plot_roc_auc(self):\n",
        "      if not self.is_model_trained:\n",
        "          raise RuntimeError(\"Model must be trained before it can be evaluated\")\n",
        "      if self.y_pred is None:\n",
        "        dtest = xgb.DMatrix(self.X_test.loc[:, self.feature_names])\n",
        "        self.y_pred = self.model.predict(dtest)\n",
        "      \n",
        "      fpr, tpr, _ = roc_curve(self.y_test, self.y_pred)\n",
        "      roc_auc = self.roc_auc()\n",
        "      plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
        "      plt.plot([0, 1], [0, 1], 'k--')\n",
        "      plt.xlabel('False Positive Rate')\n",
        "      plt.ylabel('True Positive Rate')\n",
        "      plt.title('Receiver Operating Characteristic')\n",
        "      plt.legend(loc=\"lower right\")\n",
        "      plt.show()\n",
        "\n",
        "    \n",
        "    \n",
        "    def plot_feature_importance(self, n_features =None):\n",
        "      if not self.is_model_trained:\n",
        "          raise RuntimeError(\"Model must be trained before feature importances can be evaluated\")\n",
        "\n",
        "      feature_importances = self.model.get_score(importance_type='weight')\n",
        "      if n_features is None:\n",
        "        n_features = len(feature_importances)\n",
        "\n",
        "      sorted_feature_importances = sorted(feature_importances.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "      # Extract feature names and importance scores\n",
        "      features = [x[0] for x in sorted_feature_importances]\n",
        "      importances = [x[1] for x in sorted_feature_importances]\n",
        "\n",
        "      # Create a bar plot of sorted feature importances\n",
        "      plt.figure(figsize=(8, 9.5))\n",
        "      plt.barh(features[0:(n_features-1)][::-1], \n",
        "               importances[0:(n_features-1)][::-1])\n",
        "      plt.xlabel('Importance')\n",
        "      plt.ylabel('Features')\n",
        "      plt.title(f'Top {n_features} Features')\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "    def predict_prob(self, new_dataset, threshold = None): \n",
        "      \"\"\"\n",
        "      Generate predictions from the model from new data. \n",
        "      If a threshold is provided, method will provide binary predictions. \n",
        "      If no threshold is provided (default), probabilities will be outputted \n",
        "\n",
        "      Args:\n",
        "          new_dataset (pd.DataFrame or Dataset): New data to make predictions on.\n",
        "          threshold (float, optional): Threshold for converting probabilities to binary predictions. Defaults to None \n",
        "      \"\"\"\n",
        "      ##check that the model has been trained\n",
        "      if not self.is_model_trained:\n",
        "          raise RuntimeError(\"Model must be trained before new predictions can be made\")\n",
        "      ###check that trained features are supplied in the new dataset\n",
        "      if isinstance(new_dataset, pd.DataFrame):\n",
        "        if not all(feature in new_dataset.columns for feature in self.feature_names):\n",
        "            raise ValueError(\"All feature names must be in the new dataset\")\n",
        "        data = new_dataset.loc[:, self.feature_names]\n",
        "      elif isinstance(new_dataset, Dataset): \n",
        "        if not all(feature in new_dataset.X.columns for feature in self.feature_names):\n",
        "            raise ValueError(\"All feature names must be in the new dataset\")\n",
        "        data = new_dataset.X.loc[:, self.feature_names]\n",
        "      \n",
        "      ##perform predictions\n",
        "      dtest = xgb.DMatrix(data)\n",
        "      y = self.model.predict(dtest)\n",
        "      if threshold is None:\n",
        "        return y\n",
        "      elif isinstance(threshold, float):\n",
        "        ##ensure threshold is between 0 and 1\n",
        "        threshold = max(min(float(threshold), 1), 0)\n",
        "        print(f'obtaining model class using supplied threshold:{threshold}')\n",
        "        ##convert to class\n",
        "        y_class = (y >= threshold).astype(int)\n",
        "        return y_class\n",
        "      else:\n",
        "        print('threshold must be supplied as a float between 0 and 1 - returning probabilities instead!')\n",
        "        return y\n",
        "      \n",
        "      \n",
        "\n"
      ],
      "metadata": {
        "id": "6_a5hjaErHOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "612d0620-4788-418e-dcdc-d682b8ece633"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing classes.py\n"
          ]
        }
      ]
    }
  ]
}